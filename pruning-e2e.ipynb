{"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU"},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Import libraries","metadata":{"id":"EPhVbb7-vHbr"}},{"cell_type":"code","source":"import random\nimport gc\nimport json\nfrom tqdm import tqdm\nimport os\n\nimport numpy as np\nimport pandas as pd\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader, TensorDataset\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, log_loss\n\n!pip install statsmodels\nfrom statsmodels.tsa.stattools import adfuller, grangercausalitytests\nfrom statsmodels.tsa.stattools import pacf\n\n!pip install shap\nimport shap\n\n!git clone https://github.com/UCMerced-ML/LC-model-compression.git\n!mv ./LC-model-compression/* ./\n!rm -r ./LC-model-compression\nimport lc\nfrom lc.torch import ParameterTorch as Param, AsVector, AsIs\nfrom lc.compression_types import ConstraintL0Pruning, ConstraintL1Pruning, ConstraintL1Pruning, LowRank, RankSelection, AdaptiveQuantization","metadata":{"id":"74W8I9_Ar6cC","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#List of all parameters","metadata":{"id":"nIqMu7Wt0hXX"}},{"cell_type":"code","source":"BATCH_SIZE = 16\nTEST_SIZE = 0.2\nSEED = 42\n\nPACF_ALPHA = 0.05\nPACF_NLAGS = 200\n\nGC_ALPHA = 0.005\nGC_MAXLAG = 10\nGC_THRESHOLD = 1e-5\n\nLEARNING_RATE = 0.005\nWEIGHT_DECAY = 0\nNUM_FEATURES = 33\nEPOCHS_PER_STEP = 15\nEPOCHS = 7500\n\nWEIGHTS_DIFF_ORDER = 2\nLOSS_DIFF_ORDER = 2\n\nloss_fn = nn.BCEWithLogitsLoss()","metadata":{"id":"qeybRo7z0lPm","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#Seed for reproducibility","metadata":{"id":"GuruP3jq0IKS"}},{"cell_type":"code","source":"random.seed(SEED)\nnp.random.seed(SEED)\nnp.random.RandomState(SEED)\ntorch.manual_seed(SEED)\nos.environ[\"CUBLAS_WORKSPACE_CONFIG\"]=\":4096:2\"","metadata":{"id":"4qgxfqjl1bi2","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Preprocessing","metadata":{"id":"I-KWcmEQvOQa"}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        df = pd.read_csv(os.path.join(dirname, \"ionosphere_data_kaggle.csv\"))\n        break\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"id":"46DzJPDjuYc5","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# feature 2 has all zeroes\ndf = df.drop(\"feature2\", axis=1)\n\n# convert labels to numeric\ndf['label'] = df['label'].apply(lambda x: 0 if x == 'g' else 1)\n\n# convert to numeric\ndf = df.apply(pd.to_numeric)\n\n# normalize the data\nfeatures = df.columns.tolist()\nfeatures.remove(\"label\")\nmean = df[features].mean()\nstd_dev = df[features].std()\ndf[features] = (df[features] - mean) / std_dev\n\n# dataset\ndata = df.to_numpy()\nx, y = data[:, :-1], np.expand_dims(data[:, -1], axis=1)","metadata":{"id":"jBghTBtbzziv","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x.shape, y.shape","metadata":{"id":"BqB0LcQkz9gB","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train and test split\nx = x.astype(np.float32)\ny = y.astype(np.float32)\n\ntrain_x, test_x, train_y, test_y = train_test_split(x, y, test_size=TEST_SIZE, random_state=SEED)\nprint(train_x.shape, train_y.shape, test_x.shape, test_y.shape)","metadata":{"id":"ZP260V-d0YK9","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# load to gpu\ndevice = torch.device(\"cuda\")\n\nx_train_tensor = torch.from_numpy(train_x).to(device)\nx_test_tensor = torch.from_numpy(test_x).to(device)\n\ny_train_tensor = torch.from_numpy(train_y).to(device)\ny_test_tensor = torch.from_numpy(test_y).to(device)","metadata":{"id":"XWAKVevy01uN","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# create dataset and dataloader\ntrain_dataset = TensorDataset(x_train_tensor, y_train_tensor)\ntrain_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=False)","metadata":{"id":"Oj0kh6Xp0etm","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#Utility functions","metadata":{"id":"81llVx7v1s3L"}},{"cell_type":"code","source":"def save_shap_values(shap_values, model_name):\n  values = np.abs(shap_values.values).mean(axis = 0)\n  val_dict = dict()\n  for i,val in enumerate(values):\n    val_dict[i] = val\n\n  with open(model_name, 'w') as f:\n    json.dump(val_dict, f)","metadata":{"id":"n0wzbO1C1tci","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_pacf_order(time_data, nlags = PACF_NLAGS):\n  print(nlags)\n  pacf_arr, confid_arr = pacf(time_data, nlags, alpha = PACF_ALPHA)\n  for i in range(nlags):\n    if abs(pacf_arr[i]) < 0.05:\n        print(f\"pacf order is {max(i - 1,1)}\")\n        return max(i - 1,1)\n  print(f\"pacf order is 1\")\n  return 1","metadata":{"id":"3OL-ACNx1yVk","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def check_granger_causality(weights_dict, alpha = GC_ALPHA):\n  maxlag = GC_MAXLAG\n  tests = [\"params_ftest\", \"ssr_ftest\", \"ssr_chi2test\", \"lrtest\"]\n\n  gc_masks = dict()\n  loss_series = weights_dict[\"loss\"]\n  order = get_pacf_order(loss_series.reshape(-1,1) , nlags = PACF_NLAGS)\n  print(order)\n\n  for key in weights_dict.keys():\n    if key == \"loss\":\n      continue\n    mask = []\n    for index, time_series in tqdm(weights_dict[key].items()):\n      try:\n        data = np.concatenate([loss_series.reshape(-1,1), time_series.reshape(-1,1)], axis = 1)\n        gc_test = grangercausalitytests(data, [order], verbose = False)\n        p_value = max([gc_test[order][0][test][1] for test in tests])\n        if p_value < alpha:\n          mask.append(1)\n        else:\n          mask.append(0)\n      except:\n        mask.append(1)\n    gc_masks[key] = mask\n  return gc_masks","metadata":{"id":"DDi_64zb2MyH","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def compare_feature_importances(file1, file2):\n  with open(file1, 'r') as f:\n    imp1 = json.load(f)\n  with open(file2, 'r') as f:\n    imp2 = json.load(f)\n\n  sort1 = sorted(imp1.values(), reverse = True)\n  sum1 = sum(sort1)\n  sort2 = sorted(imp2.values(), reverse = True)\n  sum2 = sum(sort2)\n\n  total_diff = 0\n\n  assert len(imp1) == len(imp2)\n  for (key1, val1), (key2, val2) in zip(imp1.items(), imp2.items()):\n    rank1 = sort1.index(val1)\n    rank2 = sort2.index(val2)\n    diff = abs(rank1 - rank2)\n    diff_prop = abs(val1/sum1 - val2/sum2)\n\n    print(key1, key2, diff, diff_prop)\n\n    total_diff += diff * diff_prop\n\n  return total_diff, total_diff/len(sort1)","metadata":{"id":"sWllqLsF6kYj","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def evaluate_performance(net):\n    y_pred_train = net.infer(x_train_tensor)\n    loss_train = log_loss(train_y, y_pred_train)\n    acc_train = accuracy_score(train_y, y_pred_train > 0.5)\n\n    y_pred_test = net.infer(x_test_tensor)\n    loss_test = log_loss(test_y, y_pred_test)\n    acc_test = accuracy_score(test_y, y_pred_test > 0.5)\n\n    print(f\"loss_train {loss_train}\")\n    print(f\"acc_train {acc_train}\")\n    print(f\"loss_test {loss_test}\")\n    print(f\"acc_test {acc_test}\")","metadata":{"id":"zXq_qKNZ7nX9","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def my_l_step(lc_model, lc_penalty, step, lr=LEARNING_RATE):\n    params = list(filter(lambda p: p.requires_grad, lc_model.parameters()))\n    optimizer = torch.optim.SGD(params, lr, weight_decay=WEIGHT_DECAY)\n\n    print(f'\\nL-step #{step} with lr: {lr:.5f}')\n\n    epochs_per_step_ = EPOCHS_PER_STEP\n    avg_loss = 0\n    for _ in range(epochs_per_step_):\n        y_pred = lc_model(x_train_tensor)\n        loss = loss_fn(y_pred, y_train_tensor) + lc_penalty()\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        avg_loss += loss.item()\n\n    if epochs_per_step_ != 0:\n      print(f'avg lc loss {avg_loss / epochs_per_step_:.5f}')\n    evaluate_performance(lc_model)","metadata":{"id":"U1hz1srn7Jw_","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def my_l_step_batching(lc_model, lc_penalty, step, lr=LEARNING_RATE):\n    params = list(filter(lambda p: p.requires_grad, lc_model.parameters()))\n    optimizer = torch.optim.SGD(params, lr, weight_decay=WEIGHT_DECAY)\n\n    print(f'\\nL-step #{step} with lr: {lr:.5f}')\n\n    epochs_per_step_ = EPOCHS_PER_STEP\n    avg_loss = 0\n    for _ in range(epochs_per_step_):\n        for x, y in train_loader:\n            y_pred = lc_model(x)\n            loss = loss_fn(y_pred, y) + lc_penalty()\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            avg_loss += loss.item()\n\n    if epochs_per_step_ != 0:\n      print(f'avg lc loss {avg_loss / epochs_per_step_:.5f}')\n    evaluate_performance(lc_model)","metadata":{"id":"I0ztxzvuAptc","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def make_stationary(weights_dict):\n  stationary_weights = dict()\n  stationary_weights[\"loss\"] = np.diff(weights_dict[\"loss\"], n = LOSS_DIFF_ORDER).reshape(-1)\n\n  for key in list(weights_dict.keys())[:-1]:\n    stationary_weights[key] = dict()\n    weights = np.array(weights_dict[key])\n\n    for i in range(weights.shape[-1]):\n      time_series = weights[:,i]\n      time_series = np.diff(time_series, n = WEIGHTS_DIFF_ORDER)\n      stationary_weights[key][i] = time_series\n  return stationary_weights","metadata":{"id":"-2EQznwiBGda","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# The Model","metadata":{"id":"_nyLSJs_Cl3Z"}},{"cell_type":"code","source":"class neural_network(nn.Module):\n  def __init__(self, num_features):\n    super(neural_network, self).__init__()\n\n    self.layers = nn.Sequential(\n        nn.Linear(num_features, 64),\n        nn.ReLU(),\n        nn.Linear(64, 64),\n        nn.ReLU(),\n        nn.Linear(64, 64),\n        nn.ReLU(),\n        nn.Linear(64, 64),\n        nn.ReLU(),\n        nn.Linear(64, 1)\n    )\n\n    self.sigmoid = nn.Sigmoid()\n\n  def forward(self, x):\n    self.train()\n    return self.layers(x)\n\n  def infer(self, x):\n    self.eval()\n    with torch.no_grad():\n      if type(x) is np.ndarray: # only pass np.ndarray if model is on cpu\n        x = torch.from_numpy(x.astype(np.float32)).cuda()\n      x = self.forward(x)\n      x = self.sigmoid(x).cpu().detach()\n      return x","metadata":{"id":"X5C54lBqCntW","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# More functions","metadata":{"id":"3ah3hxnCJOnR"}},{"cell_type":"code","source":"def vanilla_training_no_batching(model):\n    # define the optimizer\n    optimizer = torch.optim.SGD(model.parameters(), lr = LEARNING_RATE, weight_decay = WEIGHT_DECAY)\n\n    # record the weights\n    recorded_weights = dict()\n    for parameter in model.named_parameters():\n        recorded_weights[parameter[0]] = []\n    recorded_weights[\"loss\"] = []\n\n    # training the vanilla model\n    print(\"Starting Vanilla Training(no batching).....\")\n\n\n    for epoch in range(EPOCHS):\n      y_pred = model(x_train_tensor)\n      loss = loss_fn(y_pred, y_train_tensor)\n      optimizer.zero_grad()\n      loss.backward()\n      optimizer.step()\n\n      for parameter in model.named_parameters():\n          name = parameter[0]\n          value = parameter[1].cpu().detach().view(-1).numpy()\n          recorded_weights[name].append(value)\n\n      recorded_weights[\"loss\"].append(loss.item())\n\n      if (epoch+1) % 500 == 0:\n          print(f\"Epoch - {epoch + 1}: Loss - {loss.item()}\")\n          evaluate_performance(model)\n\n    # epochs completed, evaluate the model once\n    print(f\"EPOCHS COMPLETED!\")\n\n    print(\"model performance after vanilla training(no batching)\")\n    evaluate_performance(model)\n\n    shap_explainer = shap.Explainer(model.infer, train_x)\n    shap_values = shap_explainer(test_x)\n    shap.plots.bar(shap_values, max_display=99)\n\n    save_shap_values(shap_values, f\"unpruned_lr_{LEARNING_RATE}_wd_{WEIGHT_DECAY}_epochs_{EPOCHS}_no_batching.json\")\n    print(f\"Saved unpruned model to -> unpruned_lr_{LEARNING_RATE}_wd_{WEIGHT_DECAY}_epochs_{EPOCHS}_no_batching.json\")\n\n    return recorded_weights, model","metadata":{"id":"-x9bQ1SMI3AJ","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def vanilla_training_batching(model):\n    # define the optimizer\n    optimizer = torch.optim.SGD(model.parameters(), lr = LEARNING_RATE, weight_decay = WEIGHT_DECAY)\n\n    # record the weights\n    recorded_weights = dict()\n    for parameter in model.named_parameters():\n        recorded_weights[parameter[0]] = []\n    recorded_weights[\"loss\"] = []\n\n    # training the vanilla model\n    print(\"Starting Vanilla Training(batching).....\")\n\n\n    for epoch in range(EPOCHS):\n      batch_loss = 0\n\n      for x, y in train_loader:\n        y_pred = model(x)\n        loss = loss_fn(y_pred, y)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        batch_loss += loss\n\n      loss = batch_loss\n      for parameter in model.named_parameters():\n          name = parameter[0]\n          value = parameter[1].cpu().detach().view(-1).numpy()\n          recorded_weights[name].append(value)\n\n      recorded_weights[\"loss\"].append(loss.item())\n\n      if (epoch+1) % 500 == 0:\n          print(f\"Epoch - {epoch + 1}: Loss - {loss.item()}\")\n          evaluate_performance(model)\n\n    # epochs completed, evaluate the model once\n    print(f\"EPOCHS COMPLETED!\")\n\n    print(\"model performance after vanilla training(batching)\")\n    evaluate_performance(model)\n\n    shap_explainer = shap.Explainer(model.infer, train_x)\n    shap_values = shap_explainer(test_x)\n    shap.plots.bar(shap_values, max_display=99)\n\n    save_shap_values(shap_values, f\"unpruned_lr_{LEARNING_RATE}_wd_{WEIGHT_DECAY}_epochs_{EPOCHS}_batching.json\")\n    print(f\"Saved unpruned model to -> unpruned_lr_{LEARNING_RATE}_wd_{WEIGHT_DECAY}_epochs_{EPOCHS}_batching.json\")\n\n    return recorded_weights, model","metadata":{"id":"hhrsT5rNbRPX","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def gc_prune(model, recorded_weights, batching=False):\n    print(\"Starting Granger Causal Pruning.....\\n\\n\")\n\n    weights = make_stationary(recorded_weights)\n    masks = check_granger_causality(weights)\n\n    model.cpu()\n    for name, parameter in model.named_parameters():\n      mask = torch.Tensor(masks[name]).view(parameter.shape)\n      parameter.data = torch.mul(mask, parameter.data)\n\n    model.cuda()\n    shap_explainer = shap.Explainer(model.infer, train_x)\n    shap_values = shap_explainer(test_x)\n    shap.plots.bar(shap_values, max_display=99)\n\n    gc_mask = dict()\n    for name, parameter in model.named_parameters():\n      parameter = parameter.cpu().detach().view(-1)\n      mask = (parameter.abs() < GC_THRESHOLD).int()\n      gc_mask[name] = mask\n\n    pruned = 0\n    total = 0\n    for key in gc_mask:\n      count = sum(gc_mask[key])\n      pruned += count\n      total += len(gc_mask[key])\n\n    print(\"Model performance after gc training\")\n    evaluate_performance(model)\n    print(f\"\\n\\nGC Pruning completed, percentage of model pruned is: {pruned * 100 /total}\")\n\n    if not batching:\n      save_shap_values(shap_values, f\"gc_lr_{LEARNING_RATE}_wd_{WEIGHT_DECAY}_epochs_{EPOCHS}_no_batching.json\")\n      print(f\"Saved gc pruned model to -> gc_lr_{LEARNING_RATE}_wd_{WEIGHT_DECAY}_epochs_{EPOCHS}_no_batching.json\")\n\n    else:\n      save_shap_values(shap_values, f\"gc_lr_{LEARNING_RATE}_wd_{WEIGHT_DECAY}_epochs_{EPOCHS}_batching.json\")\n      print(f\"Saved gc pruned model to -> gc_lr_{LEARNING_RATE}_wd_{WEIGHT_DECAY}_epochs_{EPOCHS}_batching.json\")\n\n    return round((pruned / total).item(), 3)\n","metadata":{"id":"c9LJy9E-MBJC","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def lc_no_batching(prune_total):\n  lc_model = neural_network(train_x.shape[1]).to(device)\n  model_size = sum(p.numel() for p in lc_model.parameters() if p.requires_grad)\n\n  mu_s = [1 for i in range(EPOCHS // EPOCHS_PER_STEP)]\n  layers = [lambda x=x: getattr(x, 'weight') for x in lc_model.modules() if isinstance(x, nn.Linear)]\n  compression_tasks = {\n    Param(layers, device): (AsVector, ConstraintL0Pruning(kappa=int(model_size * (1 - prune_total))), 'pruning')\n  }\n\n  lc_alg = lc.Algorithm(\n    model=lc_model,                            # model to compress\n    compression_tasks=compression_tasks,  # specifications of compression\n    l_step_optimization=my_l_step,        # implementation of L-step\n    mu_schedule=mu_s,                     # schedule of mu values\n    evaluation_func=evaluate_performance      # evaluation function\n  )\n\n  lc_alg.run()\n\n  print(\"LC Pruning Training Completed!\")\n\n  lc_model.cuda()\n  shap_explainer = shap.Explainer(lc_model.infer, train_x)\n  shap_values = shap_explainer(test_x)\n  shap.plots.bar(shap_values, max_display=99)\n\n  print(\"Model Performance after LC Training\")\n  evaluate_performance(lc_model)\n\n  lc_mask = dict()\n  for name, parameter in lc_model.named_parameters():\n    parameter = parameter.cpu().detach().view(-1)\n    mask = (parameter.abs() < GC_THRESHOLD).int()\n    lc_mask[name] = mask\n\n  pruned = 0\n  total = 0\n  for key in lc_mask:\n    count = sum(lc_mask[key]).data\n    pruned += count\n    total += len(lc_mask[key])\n\n  print(f\"LC Pruning completed, percentage of model pruned is: {round((pruned / total).item(), 3)}\")\n  save_shap_values(shap_values, f\"lc_lr_{LEARNING_RATE}_wd_{WEIGHT_DECAY}_epochs_{EPOCHS}_no_batching.json\")\n  print(f\"Saved lc pruned model to -> lc_lr_{LEARNING_RATE}_wd_{WEIGHT_DECAY}_epochs_{EPOCHS}_no_batching.json\")","metadata":{"id":"1L6p9oOOP6bb","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def lc_batching(prune_total):\n  lc_model = neural_network(train_x.shape[1]).to(device)\n  model_size = sum(p.numel() for p in lc_model.parameters() if p.requires_grad)\n\n  mu_s = [1 for i in range(EPOCHS // EPOCHS_PER_STEP)]\n  layers = [lambda x=x: getattr(x, 'weight') for x in lc_model.modules() if isinstance(x, nn.Linear)]\n  compression_tasks = {\n    Param(layers, device): (AsVector, ConstraintL0Pruning(kappa=int(model_size * (1 - prune_total))), 'pruning')\n  }\n\n  lc_alg = lc.Algorithm(\n    model=lc_model,                            # model to compress\n    compression_tasks=compression_tasks,  # specifications of compression\n    l_step_optimization=my_l_step_batching,        # implementation of L-step\n    mu_schedule=mu_s,                     # schedule of mu values\n    evaluation_func=evaluate_performance      # evaluation function\n  )\n\n  lc_alg.run()\n\n  print(\"LC Pruning Training Completed!\")\n\n  lc_model.cuda()\n  shap_explainer = shap.Explainer(lc_model.infer, train_x)\n  shap_values = shap_explainer(test_x)\n  shap.plots.bar(shap_values, max_display=99)\n\n  print(\"Model Performance after LC Training\")\n  evaluate_performance(lc_model)\n\n  lc_mask = dict()\n  for name, parameter in lc_model.named_parameters():\n    parameter = parameter.cpu().detach().view(-1)\n    mask = (parameter.abs() < GC_THRESHOLD).int()\n    lc_mask[name] = mask\n\n  pruned = 0\n  total = 0\n  for key in lc_mask:\n    count = sum(lc_mask[key]).data\n    pruned += count\n    total += len(lc_mask[key])\n\n  print(f\"LC Pruning completed, percentage of model pruned is: {round((pruned / total).item(), 3)}\")\n  save_shap_values(shap_values, f\"lc_lr_{LEARNING_RATE}_wd_{WEIGHT_DECAY}_epochs_{EPOCHS}_batching.json\")\n  print(f\"Saved lc pruned model to -> lc_lr_{LEARNING_RATE}_wd_{WEIGHT_DECAY}_epochs_{EPOCHS}_batching.json\")","metadata":{"id":"LFOmCgjtcbOc","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#Main Training Functions","metadata":{"id":"EHZbNQkyEyoe"}},{"cell_type":"code","source":"def train_no_batching():\n\n    # initialize the model\n    model = neural_network(num_features = NUM_FEATURES).to(device)\n\n    # vanilla training\n    recorded_weights, model = vanilla_training_no_batching(model)\n\n    # granger causal pruning\n    pruned_total = gc_prune(model, recorded_weights, batching=False)\n\n    # lc pruning\n    lc_no_batching(pruned_total)\n\n    # comparison\n    print(\"GC Pruning Comparison\")\n    print(compare_feature_importances(f\"unpruned_lr_{LEARNING_RATE}_wd_{WEIGHT_DECAY}_epochs_{EPOCHS}_no_batching.json\", f\"gc_lr_{LEARNING_RATE}_wd_{WEIGHT_DECAY}_epochs_{EPOCHS}_no_batching.json\"))\n\n    print(\"LC Pruning Comparison\")\n    print(compare_feature_importances(f\"unpruned_lr_{LEARNING_RATE}_wd_{WEIGHT_DECAY}_epochs_{EPOCHS}_no_batching.json\", f\"lc_lr_{LEARNING_RATE}_wd_{WEIGHT_DECAY}_epochs_{EPOCHS}_no_batching.json\"))","metadata":{"id":"gklbYxcFEzvB","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_batching():\n    # initialize the model\n    model = neural_network(num_features = NUM_FEATURES).to(device)\n\n    # vanilla training\n    recorded_weights, model = vanilla_training_batching(model)\n\n    # granger causal pruning\n    pruned_total = gc_prune(model, recorded_weights, batching=True)\n\n    # lc pruning\n    lc_batching(pruned_total)\n\n    # comparison\n    print(\"GC Pruning Comparison\")\n    print(compare_feature_importances(f\"unpruned_lr_{LEARNING_RATE}_wd_{WEIGHT_DECAY}_epochs_{EPOCHS}_batching.json\", f\"gc_lr_{LEARNING_RATE}_wd_{WEIGHT_DECAY}_epochs_{EPOCHS}_batching.json\"))\n\n    print(\"LC Pruning Comparison\")\n    print(compare_feature_importances(f\"unpruned_lr_{LEARNING_RATE}_wd_{WEIGHT_DECAY}_epochs_{EPOCHS}_batching.json\", f\"lc_lr_{LEARNING_RATE}_wd_{WEIGHT_DECAY}_epochs_{EPOCHS}_batching.json\"))","metadata":{"id":"TWUYHqVmYVnr","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_no_batching()","metadata":{"id":"zXsatQWXdI3M","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train_batching()","metadata":{"id":"2wp7MKzWdKzH","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#MAIN","metadata":{"id":"NVhFFRVsuN9e"}},{"cell_type":"code","source":"def main():\n  if __name__ == \"__main__\":\n    train_batching()\n    train_no_batching()","metadata":{"id":"aSOQMOMluPGj","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# main()","metadata":{"id":"XOUgS4e4uTne","trusted":true},"execution_count":null,"outputs":[]}]}